{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python385jvsc74a57bd01baa965d5efe3ac65b79dfc60c0d706280b1da80fedb7760faf2759126c4f253",
   "display_name": "Python 3.8.5 64-bit"
  },
  "metadata": {
   "interpreter": {
    "hash": "1baa965d5efe3ac65b79dfc60c0d706280b1da80fedb7760faf2759126c4f253"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "import csv\n",
    "\n",
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.models\n",
    "from torchsummary import summary\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.utils.data as data\n",
    "import json\n",
    "from torchvision.datasets import ImageFolder\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "##### Efficient Net V1\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "\n",
    "try:\n",
    "    from tqdm.notebook import tqdm\n",
    "except ImportError:\n",
    "    print('tqdm could not be imported. If you want to use progress bar during training,'\n",
    "          'install tqdm from https://github.com/tqdm/tqdm.')"
   ]
  },
  {
   "source": [
    "# File Path"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_img_path = './train_images'\n",
    "test_img_path = './test_images'\n",
    "train_label_file = 'train.csv'\n",
    "test_label_file = 'test.csv'"
   ]
  },
  {
   "source": [
    "# Read list"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_img = []\n",
    "train_label = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# img list\n",
    "for file in os.listdir(train_img_path):\n",
    "    train_img.append(train_img_path + '/' + file)\n",
    "\n",
    "# read label\n",
    "with open(train_label_file, newline = '') as csvfile:\n",
    "    rows = csv.reader(csvfile)\n",
    "    for i, row in enumerate(rows):\n",
    "        if i: train_label.append(int(row[1]))\n",
    "train_label = np.array(train_label)"
   ]
  },
  {
   "source": [
    "# Custom Dataset"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AOIDataset(Dataset):\n",
    "    def __init__(self, img_list, label_list = None, transform = None):\n",
    "        self.img_list = img_list\n",
    "        self.label = label_list\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Read img\n",
    "        img = Image.open(self.img_list[index])\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        if len(self.label):\n",
    "            return img, self.label[index]\n",
    "        return img\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_list)"
   ]
  },
  {
   "source": [
    "# 超參數"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_rate = 0.5\n",
    "BATCH_SIZE = 24\n",
    "num_workers = 0\n",
    "Epoch = 100\n",
    "lr = 1e-4\n",
    "img_size = 224\n",
    "patience = 8 # 耐心程度"
   ]
  },
  {
   "source": [
    "# dataloader"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.Resize((img_size, img_size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,)),\n",
    "])\n",
    "\n",
    "valid_transform = transforms.Compose([\n",
    "    transforms.Resize((img_size, img_size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(train_img, train_label, test_size=1-split_rate, random_state=42)\n",
    "\n",
    "train_dataset = AOIDataset(img_list = X_train, label_list = y_train,\n",
    "                    transform = transform)\n",
    "\n",
    "valid_dataset = AOIDataset(img_list = X_test, label_list = y_test,\n",
    "                    transform = valid_transform )\n",
    "\n",
    "train_set_size = len(train_dataset)\n",
    "valid_set_size = len(valid_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(\n",
    "    train_dataset, batch_size=BATCH_SIZE, shuffle=True, pin_memory=True, num_workers=num_workers)\n",
    "\n",
    "valid_dataloader = DataLoader(\n",
    "    valid_dataset, batch_size=64, pin_memory=True, num_workers=num_workers)"
   ]
  },
  {
   "source": [
    "# Training"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "VGG(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU(inplace=True)\n",
       "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): ReLU(inplace=True)\n",
       "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (18): ReLU(inplace=True)\n",
       "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): ReLU(inplace=True)\n",
       "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): ReLU(inplace=True)\n",
       "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (25): ReLU(inplace=True)\n",
       "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (27): ReLU(inplace=True)\n",
       "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (29): ReLU(inplace=True)\n",
       "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): Dropout(p=0.5, inplace=False)\n",
       "    (6): Linear(in_features=4096, out_features=6, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "# Efficient Net V1 B0\n",
    "# model = EfficientNet.from_pretrained(\n",
    "#             'efficientnet-b0', in_channels=1, num_classes=6)\n",
    "model = torchvision.models.vgg16(pretrained = True)\n",
    "\n",
    "# model = torch.nn.Sequential(\n",
    "#       nn.Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "#       nn.ReLU(inplace=True),\n",
    "#       nn.Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "#       nn.ReLU(inplace=True),\n",
    "#       nn.MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False),\n",
    "#       nn.ReLU(),\n",
    "#       nn.Flatten(),\n",
    "#       nn.Linear(in_features=262144, out_features=4096, bias=True),\n",
    "#       nn.ReLU(inplace=True),\n",
    "#       nn.Dropout(p=0.5, inplace=False),\n",
    "#       nn.Linear(in_features=4096, out_features=4096, bias=True),\n",
    "#       nn.ReLU(inplace=True),\n",
    "#       nn.Dropout(p=0.5, inplace=False),\n",
    "#       nn.Linear(in_features=4096, out_features=6, bias=True),\n",
    "#     )\n",
    "# for param in model.parameters():\n",
    "#     param.requires_grad = False\n",
    "\n",
    "\n",
    "model.features = torch.nn.Sequential(\n",
    "    torch.nn.Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "    torch.nn.ReLU(inplace=True),\n",
    "    torch.nn.Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "    torch.nn.ReLU(inplace=True),\n",
    "    torch.nn.MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False),\n",
    "    torch.nn.Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "    torch.nn.ReLU(inplace=True),\n",
    "    torch.nn.Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "    torch.nn.ReLU(inplace=True),\n",
    "    torch.nn.MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False),\n",
    "    torch.nn.Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "    torch.nn.ReLU(inplace=True),\n",
    "    torch.nn.Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "    torch.nn.ReLU(inplace=True),\n",
    "    torch.nn.Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "    torch.nn.ReLU(inplace=True),\n",
    "    torch.nn.MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False),\n",
    "    torch.nn.Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "    torch.nn.ReLU(inplace=True),\n",
    "    torch.nn.Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "    torch.nn.ReLU(inplace=True),\n",
    "    torch.nn.Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "    torch.nn.ReLU(inplace=True),\n",
    "    torch.nn.MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False),\n",
    "    torch.nn.Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "    torch.nn.ReLU(inplace=True),\n",
    "    torch.nn.Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "    torch.nn.ReLU(inplace=True),\n",
    "    torch.nn.Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "    torch.nn.ReLU(inplace=True),\n",
    "    torch.nn.MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False),\n",
    ")\n",
    "\n",
    "model.classifier = torch.nn.Sequential(\n",
    "    torch.nn.Linear(in_features=25088, out_features=4096, bias=True),\n",
    "    torch.nn.ReLU(inplace=True),\n",
    "    torch.nn.Dropout(p=0.5, inplace=False),\n",
    "    torch.nn.Linear(in_features=4096, out_features=4096, bias=True),\n",
    "    torch.nn.ReLU(inplace=True),\n",
    "    torch.nn.Dropout(p=0.5, inplace=False),\n",
    "    torch.nn.Linear(in_features=4096, out_features=6, bias=True),\n",
    "  )\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = optim.AdamW(model.parameters(), lr = lr)\n",
    "# optimizer = optim.SGD(model.parameters(),\n",
    "#                     lr=lr,\n",
    "#                     weight_decay=1e-5,\n",
    "#                     momentum=0.9,\n",
    "#                     nesterov=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max',factor=0.1, patience=patience, verbose=True, threshold=1e-4, min_lr=1e-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/53 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5499454dafbc455ebbf7b088067a3f1a"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/20 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "326447ee10394497b57fc458c713878c"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch:1 Train Loss:0.0707,  Train Accuracy:0.2524,  Validation Loss:0.0264,  Validation Accuracy:0.2650\nTraining time is:0m 58s\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/53 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1f14d1e8dd324f40be872ce1297540a1"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/20 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f4c9e8553eb8483fbe7a53a2877699d2"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch:2 Train Loss:0.0702,  Train Accuracy:0.2571,  Validation Loss:0.0264,  Validation Accuracy:0.2650\nTraining time is:0m 16s\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/53 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9d0ae943a9b443fba4f485762f9fa90a"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/20 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c6d40d4104314148bd89caceda2a0797"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch:3 Train Loss:0.0697,  Train Accuracy:0.2650,  Validation Loss:0.0264,  Validation Accuracy:0.2650\nTraining time is:0m 16s\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/53 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b3481ca6242d4a47a4424d81e4ef3075"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/20 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b2c5eeaa5e914489829a6599cc2e79a0"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch:4 Train Loss:0.0698,  Train Accuracy:0.2666,  Validation Loss:0.0262,  Validation Accuracy:0.2650\nTraining time is:0m 16s\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/53 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9771ae1340cd4290820b4e25a053ee06"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/20 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a4e334354ff744d29f276f3b370be581"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch:5 Train Loss:0.0698,  Train Accuracy:0.2381,  Validation Loss:0.0262,  Validation Accuracy:0.2722\nTraining time is:0m 16s\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/53 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f901e3faae39465298045902ca0ddf6d"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/20 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "002964eace4a43beb787c90a2d1c6c7d"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch:6 Train Loss:0.0698,  Train Accuracy:0.2492,  Validation Loss:0.0262,  Validation Accuracy:0.2650\nTraining time is:0m 16s\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/53 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "57d6831fc6ba4e94ba86f53d09419067"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/20 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8a3c338da7134d1f8dd97d3a1c097a3e"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch:7 Train Loss:0.0697,  Train Accuracy:0.2642,  Validation Loss:0.0262,  Validation Accuracy:0.2650\nTraining time is:0m 16s\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/53 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "aa67bb5315914ef784800392cc26a930"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/20 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1e642357a6c344db87f99e174ddd81ef"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch:8 Train Loss:0.0698,  Train Accuracy:0.2714,  Validation Loss:0.0262,  Validation Accuracy:0.2650\nTraining time is:0m 16s\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/53 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0b287a4f63464ecfa3b0341ab28bd98f"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/20 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6b4ec9b32c0040068847c368c302f8c7"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch:9 Train Loss:0.0698,  Train Accuracy:0.2532,  Validation Loss:0.0262,  Validation Accuracy:0.2650\nTraining time is:0m 16s\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/53 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8589d4af4fbe44a09dcf9d59de31c61b"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/20 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "11f3982952384567acfd2ca055a9d6f8"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch:10 Train Loss:0.0697,  Train Accuracy:0.2508,  Validation Loss:0.0262,  Validation Accuracy:0.2650\nTraining time is:0m 16s\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/53 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0c4e39a3adcc400993b1920ac7d9876e"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/20 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9491e0b597414ed88dfd7aba0af8e3f6"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch:11 Train Loss:0.0700,  Train Accuracy:0.2587,  Validation Loss:0.0262,  Validation Accuracy:0.2722\nTraining time is:0m 16s\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/53 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5ccf398474914ee690a290a3329dfc85"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/20 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "09a8d7bba49a4ecaa2a603ae2af96218"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch:12 Train Loss:0.0696,  Train Accuracy:0.2658,  Validation Loss:0.0263,  Validation Accuracy:0.2650\nTraining time is:0m 16s\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/53 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c00ecd4756c84c8fa1cada0ca6215320"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/20 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c78d29b288c34d78bca7b2cce04338a6"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch:13 Train Loss:0.0697,  Train Accuracy:0.2682,  Validation Loss:0.0262,  Validation Accuracy:0.2650\nTraining time is:0m 16s\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/53 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "dbc3ed91dccf45efb7affc9ca188f7eb"
      }
     },
     "metadata": {}
    }
   ],
   "source": [
    "result_param = {'training_loss': [], 'training_accuracy': [],\n",
    "                    'validation_loss': [], 'validation_accuracy': []}\n",
    "\n",
    "for epoch in range(Epoch):\n",
    "\n",
    "    since = time.time()\n",
    "    running_training_loss = 0\n",
    "    running_training_correct = 0\n",
    "    running_valid_loss = 0\n",
    "    running_valid_correct = 0\n",
    "    model.train()\n",
    "    train_bar = tqdm(train_dataloader)\n",
    "    for imgs, label in train_bar:\n",
    "        imgs = imgs.to(device)\n",
    "        label = label.to(device, dtype=torch.long)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        out = model(imgs)\n",
    "        loss_val = loss(out, label)\n",
    "        _, pred_class = torch.max(out.data, 1)\n",
    "\n",
    "        running_training_correct += torch.sum(pred_class == label)\n",
    "        running_training_loss += loss_val\n",
    "\n",
    "        loss_val.backward()\n",
    "        optimizer.step()\n",
    "        train_bar.set_description(desc='[%d/%d] | Train Loss:%.4f' %\n",
    "                                        (epoch + 1, Epoch, loss_val.item()))\n",
    "\n",
    "    with torch.no_grad():\n",
    "            model.eval()\n",
    "            val_bar = tqdm(valid_dataloader)\n",
    "            for imgs, label in val_bar:\n",
    "                imgs = imgs.to(device)\n",
    "                label = label.to(device, dtype=torch.long)\n",
    "\n",
    "                out = model(imgs)\n",
    "                loss_val = loss(out, label)\n",
    "\n",
    "                val_bar.set_description(desc='[%d/%d] | Validation Loss:%.4f' % (epoch + 1, Epoch, loss_val.item()))\n",
    "                _, pred_class = torch.max(out.data, 1)\n",
    "                running_valid_correct += torch.sum(pred_class == label)\n",
    "                running_valid_loss += loss_val\n",
    "    \n",
    "    valid_acc = running_valid_correct.item() /  valid_set_size\n",
    "\n",
    "    result_param['training_loss'].append(\n",
    "            running_training_loss.item() / train_set_size)\n",
    "    result_param['training_accuracy'].append(running_training_correct.item() /\n",
    "                                                 train_set_size)\n",
    "    result_param['validation_loss'].append(\n",
    "            running_valid_loss.item() / valid_set_size)\n",
    "    result_param['validation_accuracy'].append(valid_acc)\n",
    "\n",
    "    scheduler.step(valid_acc)\n",
    "\n",
    "    print(\n",
    "        \"Epoch:{} Train Loss:{:.4f},  Train Accuracy:{:.4f},  Validation Loss:{:.4f},  Validation Accuracy:{:.4f}\".format(\n",
    "                epoch + 1, result_param['training_loss'][-1], result_param['training_accuracy'][-1],\n",
    "                result_param['validation_loss'][-1], result_param['validation_accuracy'][-1]))\n",
    "\n",
    "    now_time = time.time() - since\n",
    "    print(\"Training time is:{:.0f}m {:.0f}s\".format(\n",
    "        now_time // 60, now_time % 60))\n",
    "\n",
    "    if valid_acc > 0.995:\n",
    "        print('Validation Acc is above 99.5!!')\n",
    "        break\n",
    "\n",
    "    # torch.save(model.state_dict(), str(\n",
    "    #     './checkpoints/' + METHOD + '/' + \"EPOCH_\" + str(epoch) + \".pkl\"))\n",
    "    # out_file = open(str(\n",
    "    #     './checkpoints/' + METHOD + '/' + 'result_param.json'), \"w+\")\n",
    "    # json.dump(result_param, out_file, indent=4)\n"
   ]
  },
  {
   "source": [
    "# Draw Result"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true, y_pred = [], []\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "\n",
    "    val_bar = tqdm(valid_dataset)\n",
    "    for imgs, label in val_bar:\n",
    "        imgs = imgs.unsqueeze(0).to(device)\n",
    "        y_true.append(int(label))\n",
    "\n",
    "        label = label.to(device, dtype=torch.long)\n",
    "        out = model(imgs)\n",
    "        _, pred_class = torch.max(out.data, 1)\n",
    "\n",
    "        val_bar.set_description(desc='[%d/%d]' % (i, len(valid_dataset) ) )\n",
    "        \n",
    "        y_pred.append(int(pred_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "cnf_matrix = confusion_matrix(y_true, y_pred)\n",
    "plot_confusion_matrix(cnf_matrix, classes=target_names,normalize=False,\n",
    "                    title='CNN confusion matrix')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "source": [
    "# Testing"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_img = []\n",
    "# img list\n",
    "for file in os.listdir(test_img_path):\n",
    "    test_img.append(test_img_path + '/' + file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((img_size, img_size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,)),\n",
    "])\n",
    "\n",
    "test_data = AOIDataset(img_list = test_img, label_list = [],\n",
    "                    transform = transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv(test_label_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/10142 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "641d762dd7a3402f80fb83db7d17503f"
      }
     },
     "metadata": {}
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    test_bar = tqdm(test_data)\n",
    "    for i, imgs in enumerate(test_bar):\n",
    "        imgs = imgs.unsqueeze(0).to(device)\n",
    "        out = model(imgs)\n",
    "        _, pred_class = torch.max(out.data, 1)\n",
    "        submission.loc[i, 'Label'] = str(int(pred_class))\n",
    "        test_bar.set_description(desc='[%d/%d]' % (i, len(test_data) ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                    ID  Label\n",
       "count            10142  10142\n",
       "unique           10142      6\n",
       "top     test_00360.png      0\n",
       "freq                 1   2717"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ID</th>\n      <th>Label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>10142</td>\n      <td>10142</td>\n    </tr>\n    <tr>\n      <th>unique</th>\n      <td>10142</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>top</th>\n      <td>test_00360.png</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>freq</th>\n      <td>1</td>\n      <td>2717</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "source": [
    "submission.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('submissionVGG16_512sp.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}